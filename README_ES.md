# ğŸ” ai-gateway

> Microservicio Flask que actÃºa como gateway inteligente entre Jenkins, los microservicios IA (`ai-log-analyzer`, `ai-helm-linter`, `ai-pipeline-gen`) y los modelos LLM locales (Ollama) o remotos (OpenAI). Este componente organiza el flujo de informaciÃ³n, dirige las peticiones y establece fallback entre motores de IA.

---

## ğŸšª FunciÃ³n Principal

Centraliza las llamadas desde Jenkins y despacha las peticiones a los microservicios AI del entorno `devops-ai-lab`, utilizando como backend modelos de lenguaje (LLMs) para:

- ğŸ“Š AnÃ¡lisis de logs
- ğŸ§ª ValidaciÃ³n de Helm Charts
- âš™ï¸ GeneraciÃ³n de pipelines desde texto
- ğŸ§  Registro automÃ¡tico de mensajes MCP para trazabilidad (vÃ­a `ai-mcp-server`)

---

## ğŸŒ Endpoints

### `POST /analyze-log`

Analiza registros CI/CD con LLMs.

**Payload:**
```json
{
  "log": "contenido del log",
  "mode": "ollama"
}
```
â†’ Redirige internamente a `ai-log-analyzer`.  
â†’ Si hay `prompt_path` y `response_path`, registra mensaje MCP.

---

### `POST /lint-chart`

Valida un Helm Chart mediante IA.

**Payload (multipart/form-data):**
- `chart`: archivo `.tgz` del Helm Chart
- `mode`: `"ollama"` o `"openai"`
- `strict`: `"true"` o `"false"` (opcional)

â†’ Redirige a `ai-helm-linter`  
â†’ MCP registrado automÃ¡ticamente si se genera respuesta con prompt.

---

### `POST /generate-pipeline`

Convierte lenguaje natural en definiciÃ³n de pipeline CI/CD.

**Payload:**
```json
{
  "description": "Pipeline con test y despliegue usando Helm",
  "mode": "ollama"
}
```
â†’ Llama a `ai-pipeline-gen`  
â†’ MCP registrado automÃ¡ticamente.

---

### `GET /health`

Respuesta: `200 OK`  
Sirve para verificar que el gateway estÃ¡ operativo.

---

## ğŸ“¦ Estructura del Proyecto

```
ai-gateway/
â”œâ”€â”€ app.py                 # Servidor Flask principal
â”œâ”€â”€ routes/                # Endpoints organizados por dominio
â”‚   â”œâ”€â”€ analyze_log.py
â”‚   â”œâ”€â”€ lint_chart.py
â”‚   â””â”€â”€ generate_pipeline.py
â”œâ”€â”€ clients/               # MÃ³dulos de comunicaciÃ³n
â”‚   â”œâ”€â”€ service_dispatcher.py  # RedirecciÃ³n + registro MCP
â”‚   â””â”€â”€ mcp_client.py         # EnvÃ­o directo al ai-mcp-server
â”œâ”€â”€ config.py              # ConfiguraciÃ³n centralizada
â”œâ”€â”€ requirements.txt       # Dependencias
â”œâ”€â”€ Makefile               # AutomatizaciÃ³n de despliegue
â”œâ”€â”€ Dockerfile             # Imagen de contenedor
â””â”€â”€ run_*.sh               # Scripts de test locales
```

---

## âš™ï¸ Ejecutar localmente

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python3 app.py
```

---

## ğŸ” IntegraciÃ³n con Jenkins

```groovy
def jsonPayload = '''
{
  "description": "Pipeline bÃ¡sico para test y deploy",
  "mode": "ollama"
}
'''

sh '''
curl -X POST http://ai-gateway.devops-ai.svc.cluster.local:5000/generate-pipeline   -H "Content-Type: application/json"   -d '${jsonPayload}'
'''
```

---

## ğŸ§  Inteligencia Adaptativa + MCP

- Primero intenta con **Ollama local** (modelo `mistral`)
- Si falla, redirige automÃ¡ticamente a **OpenAI GPT-4o**
- Si hay `prompt_path` y `response_path`, se registra un mensaje MCP en `ai-mcp-server`
- MCP incluye `uuid`, `timestamp`, `tags`, `summary`, etc.

---

## ğŸ” Seguridad y Futuro

- AutenticaciÃ³n por token (pendiente)
- ValidaciÃ³n de entradas y sanitizaciÃ³n
- Logging estructurado pendiente de mejorar

---

## ğŸ”® Futuros mÃ³dulos

- `/explain-error`: anÃ¡lisis de tracebacks
- `/summarize-pipeline`: resumen de definiciones CI/CD
- `/compare-logs`: comparaciÃ³n entre logs

---

## ğŸ‘¨â€ğŸ’» Autor

**Dani** â€“ [@dorado-ai-devops](https://github.com/dorado-ai-devops)

---

## ğŸ›¡ Licencia

Licencia PÃºblica General GNU v3.0